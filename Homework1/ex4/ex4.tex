\section{Hiring process}
\begin{enumerate}
	\item A consulting company can execute tasks requested from its customers either with hired personnel or with freelance workers. The set of tasks is presented on a subset S of time instants \{1, ... T\}. If task $j_t$ is assigned to a hired employee of the consulting company, the cost is given by his daily salary s. If task $j_t$ is outsourced to a freelance worker, the paid cost is $c_t$ and it depends on the specific task and time instant. A worker can be hired at any time by paying him a hiring cost C and fired at any later time by paying a severance cost S.
	\newline
	\textbf{Goal:} Design an optimal strategy that runs in polynomial time that minimizes the total cost of executing the tasks. The total cost should include the costs paid to the freelances workers and the costs paid for hiring, firing and the salaries of the hired personnel. Prove that the algorithm is correct and provide an analysis of its running time. Implement the algorithm with a programming language of your choice.
	\item Assume now that task $j_t$ requires a set $W_t \subseteq W, |W| = k$, of a constant number k of different types of workers. The company should therefore decide which types of workers to hire and which types of workers to outsource. The salary cost for any time instant is the same for all types of workers as well as the hiring and the firing cost. The cost of worker $j \in W$ varies with time: the cost of worker $j \in W$ is equal to $c_t^j$.
	\newline
	\textbf{Goal:} Design an optimal strategy that runs in polynomial time that minimizes the total cost of executing the tasks.
	\newline
	\textbf{Hint:} Use dynamic programming for both exercises. The polynomial running time of the	final algorithm in the second exercise should depend on T and on $2^k$. Start with the case k = 2 an generalize the approach from there.
\end{enumerate}

\subsection{The algorithm}
Given a sequence of tasks, ordered by period t, the algorithm creates a matrix of costs. There are 2 rows and T columns: matrix[r][c] is the minimum cost if you decide to have r workers in period c, where r is either 0 or 1. This matrix is built for each period t, considering which is the best option of the previous period that leads, with all the necessary costs, to the current one. For example, matrix[1][5] can derive from two previous cases: matrix[1][4] and matrix[0][4]; in the first case we have just to pay the daily salary, since the worker has already been hired, instead in the second one we should also pay the hiring cost.
\begin{enumerate}
	\item For every period $t \in \{1,..., T\}$ take all tasks that have to be processed in period t.
	\item There are two possible cases:
	\begin{enumerate}
		\item assign all the tasks to a worker. For sure we have to pay the salary for period t; add to it the minimum between
		\begin{itemize}
			\item matrix[0][t-1] plus the hiring cost
			\item matrix[1][t-1]
		\end{itemize} 
		\item assign all the tasks (if any) to freelancers. Pay all the outsource costs; add to this the minimum between
		\begin{itemize}
			\item matrix[0][t-1]
			\item matrix[1][t-1] plus the severance cost
		\end{itemize}
	\end{enumerate}
	\item return the minimum between matrix[0][T] and matrix[1][T]
\end{enumerate}

\subsubsection{Running time}
There are T iterations: at each of these the algorithm collects all the tasks belonging to that period and makes a binary decision, based on the costs already computed at the previous iteration. The cost per period, indeed, is O(1) since it is based on precomputed values. The overall cost of the algorithm is O(T).

\subsubsection{Source code}
I have implemented the exercise in Python language. The code is attached as ex4\_1.py.

\subsubsection{Proof of correctness}
For each period t there are two possible cases: either we have a hired worker who can process all the tasks, or we do not have him: in this case, if there is some task to be processed, it has to be assigned necessarily to freelancers. The algorithm minimizes the cost of having 0 or 1 worker for every period, basing its analysis on the previous computations. The base case is for t = 0, where the minimum cost is equal to 0 (since the first task is at least at period t=1). The algorithm finds the same solution, since it picks the minimum between 0 (no hire) and some cost $c > 0$ (hire someone and pay him). Moreover, c is the minimum cost of having paid personnel in period 0. So for period t = 0 tha algorithm correctly computes the two minimums and in particular the optimal solution. Now, suppose the algorithm correctly computes the minimum costs $opt[*][i]$ for a given period i and both the possible cases (hired worker for period i, or not). At period t = i+1, the algorithm computes the minimum between having or not a paid worker, considering \textbf{all the possible "paths"} from all the previous optimal solutions (possible hirings, firing, salaries, etc.), thus resulting in optimal solutions at period i+1.
\newline
Say the algorithm until iteration i is optimal and at iteration i+1 makes a "wrong" decision. But we know it has considered all the paths from previous optimal solutions, adding only the "necessary" cost, and nothing more. This means that some previous solution at period i was not optimal, thus resulting in an absurd. 

\subsection{The algorithm}
I have written 3 algorithms to solve the problem above: the first one (ex\_4\_normal.py) is the generalization of the algorithm 4.1 and considers multiple freelancers for each task, meaning that also freelancers have exactly one skill as the other workers. This was my original algorithm (what I have understood at beginning). This algorithm can be also reformulated in a more efficient way, iterating k times the 4.1 algorithm: this is the second algorithm (ex\_4\_optimized.py). Finally, I have written a third algorithm, considering that the outsource cost is just a fixed amount, meaning that we do not outsource the skill but the task itself! (so it's like the freelancer is a super worker with all the necessary skills): this can be also solved in $O(T \cdot 2^k)$, but not in $O(kT)$.
\subsubsection{$2^k$ generalization}
Given a sequence of tasks, ordered by period t, and a number of skills k, the algorithm creates a matrix of costs. There are $2^k$ rows and T columns: matrix[mask][c] is the minimum cost if you decide to have $\# workers(mask)$ in period c, where mask represents a combination of hired workers (e.g. "0111" means that we have hired workers with skills 1,2,3 and no hired worker with skill 0). This matrix is built for each period t, considering which is the best option of the previous period that leads, with all the necessary costs, to the current one. Again, the minimum cost has to take in consideration possible firings, hirings and outsource costs.
\begin{enumerate}
	\item For every period $t \in \{1,..., T\}$ take all tasks that have to be processed in period t.
	\item There are $2^k$ possible cases, corresponding to different masks (combinations of hired workers): in particular, we could have or not a worker with a skill $s_i, i \in \{1, ..., k\}$.
	\begin{enumerate}
		\item for each of these combinations, minimize the cost for a given mask and a given period, based on the previous costs.
	\end{enumerate}
	\item return the minimum between matrix[$mask_1$][T], matrix[$mask_2$][T], ... matrix[$mask_{2^k}$][T]
\end{enumerate}
\subsubsection{Optimized version}
This algorithm just consists of running the 4.1 k times, and summing up all the minimum costs found at the end.
\subsubsection{The variant with "super" freelancer}
The algorithm is actually quite similar to the first one: there are $2^k$ choices we can do at each iteration (period). The only difference is in the function that computes the cost of freelancers, since the outsource cost is just a fixed quantity and not a value specific per task. Moreover, outsourcing a task may lead to not hire no other worker, since all the necessary skills are "covered" by the freelancer.

\subsubsection{Running time}
The number of iterations is always T (as in 4.1). What changes is the number of possible choices we can make at each step: this is $2^k$ for both first and third algorithm. Since these choices all use precomputed values, this means that the cost per choice is $O(2^k)$, for an overall O($T \cdot 2^{2k}$) = O($T \cdot 2^k$). In the optimized version e just run k times the algorithm 4.1, thus reducing the cost to $O(kt)$.

\subsubsection{Source code}
I have implemented the exercise in Python language. The code is attached as ex4\_2.py.

\subsubsection{Proof of correctness}
This can be seen as a generalization of the previous problem. Again the algorithm explores all the possible paths that minimize for every period and every scenario (what I have called mask), computing the optimal solution. This can be proved by induction, since the base case is easily exploitable (the algorithm computes $2^k$ costs, considering only the necessary cost). Assuming that until iteration i the solution is optimal (and also all the minimum costs are optimal for that given scenario and period), and that all the possible paths from period t and period t+1 are taken into account, a non optimal solution at iteration i+1 would lead to an absurd (as in the case with k=1).
As for the optimized version, it is based on the 4.1 correctness. Since the costs per skills are independent each others (there is no super freelancer!) we can simply minimize the costs for each skill. Say the solution is not optimal: this means that, for some skills, we made a wrong decision. But this cannot be true, since we have actually minimized the cost considering one skill at time.